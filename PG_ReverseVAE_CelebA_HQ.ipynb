{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "PG-ReverseVAE-CelebA-HQ.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G7u7Ffe1iWQx"
      },
      "source": [
        "# Start of Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AXWiGxYqjZIp"
      },
      "source": [
        "## Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M-I2EXgTDknv",
        "outputId": "945ca699-072a-4f93-eab2-0cfdc2ab4aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras.datasets.mnist as mnist\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.contrib.slim as slim\n",
        "import imageio\n",
        "\n",
        "# This is needed for access the google cloud storage to save/restore the model and load the training/testing data.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RsoYNtDeuBvj",
        "colab": {}
      },
      "source": [
        "class Flags:\n",
        "    \"\"\"\n",
        "    @author: N.Gu\n",
        "    this model has 7 stages in total, each stage represents a resolution, in total the resolution setting contains:\n",
        "    4x4, 8x8, 16x16, ..., 256x256. This can be easily up- sacled to 1024x1024, as long as the 1024x1024 \n",
        "    training data is available!\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.dataset =  \"celeba\" \n",
        "        self.z_dim = 512  \n",
        "        self.size_list = [ 4, 8, 16, 32, 64, 128, 256, 512 ]\n",
        "        self.generator_channel_list =  [ 512, 512, 512, 512, 256, 128, 64 , 32 ] \n",
        "        self.discriminator_in_channel_list =  [   512, 512, 512, 512, 256, 128, 64, 32 ]\n",
        "        self.discriminator_out_channel_list  = [ 512, 512, 512, 512, 512, 256, 128, 64  ]\n",
        "        self.encoder_in_channel_list = self.discriminator_in_channel_list \n",
        "        self.encoder_out_channel_list = self.discriminator_out_channel_list\n",
        "        self.lr_list = [  1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3, 1e-3  ]\n",
        "        # for different resolution different batchsize is used, due to to the limitation of memory\n",
        "        # smaller batchsize is benefitial to the resolution transition\n",
        "        self.batch_size_list = [128*8, 64*8, 64*8, 32*8, 32*8, 16*8, 8*8, 2*8]\n",
        "\n",
        "\n",
        "        # let the model see 800k images for each stage(or substage)\n",
        "        self.num_images_per_stage = 800000\n",
        "        # the training steps for each resolution stage. Except the first resolution, all the other resolution\n",
        "        # contains \"resolution transition\"+\"resolution stabilization\" two substages\n",
        "        self.train_steps_list = [ int( self.num_images_per_stage/self.batch_size_list[0] ) ]+ \\\n",
        "                            [int( self.num_images_per_stage*2/batch_size ) for batch_size in self.batch_size_list[1:] ]\n",
        "        self.tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "        # The path to save the model on google cloud storage\n",
        "        self.model_dir = \"gs://reverse-vae-celeba-hq/model/\"\n",
        "        # The path sto load the data on GDS\n",
        "        self.data_dir = \"gs://reverse-vae-celeba-hq/data/\"\n",
        "        # How many batches sent to TPU for one run before return back to the host CPU to do some evaluation and possibly saving model\n",
        "        self.iterations_per_loop = 200\n",
        "        self.use_tpu = True\n",
        "        # use eaulized learning rate adopted by PGGAN\n",
        "        self.is_equalized_learning_rate = True\n",
        "\n",
        "FLAGS = Flags()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sQLxeFBhCbSs"
      },
      "source": [
        "## Some helper function for visulizing the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3m4D4MfOCmhF",
        "colab": {}
      },
      "source": [
        "def add_padding( x, padding_size=(8,8,8,8), padding_value = 1 ):\n",
        "    # x is a 4 d ndarray with range [0,1]\n",
        "    background = padding_value * np.ones(  [ x.shape[0], x.shape[1]+ padding_size[0]+padding_size[2], x.shape[2] + padding_size[1]+padding_size[3], x.shape[3]   ]  ).astype(np.float32)\n",
        "    background[:, padding_size[0]:-padding_size[2], padding_size[1]:-padding_size[3], : ] = x\n",
        "    padded_x = background\n",
        "    return padded_x\n",
        "\n",
        "# to convert a bulk of images into grid of images\n",
        "def make_grid(images,  ncol= None):\n",
        "\t# ncol represents the number of columns of the image grid, if ncol is None, then arrange the grid as close to a square as possible\n",
        "\t# This function always assume that the input image is RGB color space , normalized float type\n",
        "\t\n",
        "\tif np.max(images)-np.min(images) >1 :\n",
        "\t\timages = np.clip( images, -1,1 )\n",
        "\t\timages = images /2 +0.5\n",
        "\t\t\n",
        "\timage_num = images.shape[0]\n",
        "\tnum_h = None\n",
        "\tnum_w = None\n",
        "\tim_h = images.shape[1]\n",
        "\tim_w = images.shape[2]   \n",
        "\tim_c = images.shape[3]\n",
        "\tif (ncol==None):\n",
        "\t\tnum_w = int( np.ceil(np.sqrt(image_num )))\n",
        "\t\tnum_h = int( np.ceil( image_num/ num_w ))\n",
        "\telse:\n",
        "\t\tnum_w = int(ncol)\n",
        "\t\tnum_h = int( np.ceil(  image_num/num_w ))\n",
        "\n",
        "\t# create a white pannel, which is a [height, width, channel] ndarray\n",
        "\tpannel = np.ones(( num_h * im_h, num_w * im_w , im_c )).astype(np.float32)\n",
        "\n",
        "\tfor i in range( image_num ):\n",
        "\t\tstart_h = int(i / num_w) * im_h\n",
        "\t\tstart_w = (i % num_w) * im_w\n",
        "\t\tpannel[ start_h: start_h+im_h , start_w : start_w + im_w ,: ]= images[i,:,:,:]\n",
        "\treturn  pannel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iBpeS7knDHOj"
      },
      "source": [
        "## Prepeare the input_functions for tf estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VDQujBJhBgc-"
      },
      "source": [
        "Before moving futher, one should prepare 0.17 million 256x256 celeba images and store them into the GDS as stated in FLAGS.data_dir. The celeba images should be normalized to [-1,1] and stored as TFRecord. There should be 17 Tfrecords, each TfRecord contains 10000 images. The first 15 TFRecords are used for training, and the last two are used for evaluation and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zAtpHNOsvLVo",
        "colab": {}
      },
      "source": [
        "if FLAGS.dataset == \"celeba\":\n",
        "    celeba_tfrecord_dics = {\n",
        "            # 'image': tf.VarLenFeature(dtype=tf.float32),  # This is not compatible with TPU\n",
        "            'image': tf.FixedLenFeature( shape=( 1024,1024, 3 ), dtype=tf.float32), \n",
        "            'image_shape': tf.FixedLenFeature(shape=(3,), dtype=tf.int64), \n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1w0Oa0Nx15Xt"
      },
      "source": [
        "#### The input function part:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dfeUKgn6PUgG",
        "colab": {}
      },
      "source": [
        " \"\"\"the g_w is used to control when to update the generator's parameters\n",
        "       here the pattern \"0 0 0 0 1\" means training the generator once for every n_critic = 5 batches as stated in WGAN-GP\n",
        "       This setting is due to the special property of the TPU mechanism\n",
        "       In PG-ReverseVAE this usage is deactivated since n_critic = 1 as used in PGGAN, but it's good to keep it for future usage: \n",
        "       e.g explore the influence of n_critic on performance. \n",
        "\"\"\"\n",
        "def train_input_fn( dataset, stage,  batch_size ):\n",
        "    if dataset == \"celeba\":\n",
        "        dataset_x_train = tf.data.TFRecordDataset( FLAGS.data_dir+\"celeba_hq_eval.tfrecord\", compression_type='GZIP')\n",
        "        dataset_x_train = dataset_x_train.shuffle(60000).repeat()\n",
        "        pattern = np.array([0,0,0,0,1]).repeat(batch_size).astype(np.float32)\n",
        "        dataset_g_w = tf.data.Dataset.from_tensor_slices( { \"g_w\": pattern  } ).repeat()\n",
        "        dataset_output = tf.data.Dataset.from_tensor_slices( ( np.zeros( ( batch_size,1 ) ).astype(np.float32) ) ).repeat()\n",
        "        ds = tf.data.Dataset.zip(( dataset_x_train, dataset_g_w, dataset_output))\n",
        "\n",
        "        def map_func(a,b,c):\n",
        "            parsed_example = tf.parse_single_example(a, celeba_tfrecord_dics)\n",
        "            parsed_example['image'] = tf.reshape(parsed_example['image'], [1, 1024,1024,3] )*2-1   # the value is in the range of [-1,1]   \n",
        "            a = {\"x\": tf.squeeze(tf.image.resize_bilinear(  parsed_example[\"image\"]  , [FLAGS.size_list[stage],FLAGS.size_list[stage]], align_corners=True), axis =0)  }            \n",
        "            a.update(b)\n",
        "            return a, c\n",
        "        ds = ds.map(map_func).batch( batch_size, drop_remainder = True ).prefetch(buffer_size=1)\n",
        "    return ds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G7gKO1E5Q-N3",
        "colab": {}
      },
      "source": [
        "def eval_input_fn(dataset, stage,batch_size):\n",
        "    if dataset == \"celeba\":\n",
        "\n",
        "        dataset_x_eval = tf.data.TFRecordDataset( FLAGS.data_dir+\"celeba_hq_eval.tfrecord\"  , compression_type='GZIP')\n",
        "        dataset_x_eval = dataset_x_eval.shuffle(10000).repeat()\n",
        "        dataset_output = tf.data.Dataset.from_tensor_slices( ( np.zeros( ( batch_size ) ).astype(np.float32) ) ).repeat()\n",
        "        ds = tf.data.Dataset.zip(( dataset_x_eval, dataset_output))\n",
        "        def map_func(a,b):\n",
        "            parsed_example = tf.parse_single_example(a, celeba_tfrecord_dics)\n",
        "            parsed_example['image'] = tf.reshape(parsed_example['image'],  [1,1024,1024,3] )*2-1 \n",
        "            return tf.squeeze(tf.image.resize_bilinear(  parsed_example[\"image\"]  , [FLAGS.size_list[stage],FLAGS.size_list[stage]], align_corners=True), axis =0)  , b\n",
        "        ds = ds.map(map_func).batch(batch_size, drop_remainder = True).prefetch(buffer_size =1)\n",
        "    \n",
        "    return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q6pCeeWMTBp7",
        "colab": {}
      },
      "source": [
        "def predict_input_fn(dataset, stage,batch_size):\n",
        "    \"\"\"An input function for test recontruction\"\"\"\n",
        "\n",
        "    if dataset == \"celeba\":\n",
        "        dataset = tf.data.TFRecordDataset( FLAGS.data_dir+\"celeba_hq_eval.tfrecord\" , compression_type='GZIP').shuffle(1000).take(64)\n",
        "        def map_func(a):\n",
        "            parsed_example = tf.parse_single_example(a, celeba_tfrecord_dics)\n",
        "            parsed_example['image'] = tf.reshape(parsed_example['image'],  [1, 1024,1024,3] )*2-1\n",
        "            return tf.squeeze(tf.image.resize_bilinear(  parsed_example[\"image\"]  , [FLAGS.size_list[stage],FLAGS.size_list[stage]], align_corners=True), axis =0), tf.zeros(shape= (1,))\n",
        "        dataset = dataset.map( map_func )\n",
        "        dataset = dataset.batch(batch_size,drop_remainder = False)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A0iJsuImFCSr"
      },
      "source": [
        "## Design the PG-ReverseVAE model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EcwSp7aVZ0Ql"
      },
      "source": [
        "#### The metric function used for evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qoupeiiFo1_N",
        "colab": {}
      },
      "source": [
        "def metric_fn(loss_gen, loss_dis, W_dis, loss_recon_z ):\n",
        "    \"\"\"Function to return metrics for evaluation.\n",
        "    The input parameters can be arbritary\n",
        "    \"\"\"\n",
        "    return {\"loss_gen\": tf.metrics.mean(loss_gen), \n",
        "            \"loss_dis\": tf.metrics.mean(loss_dis),\n",
        "            \"wasserstein_distance\": tf.metrics.mean( W_dis ),\n",
        "            \"loss_recon_z\": tf.metrics.mean( loss_recon_z )\n",
        "            }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FTK_3qCSZ648"
      },
      "source": [
        "#### The model structure part:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OS2beMJl85u",
        "colab": {}
      },
      "source": [
        "def pixelnorm( net ):\n",
        "    return tf.divide(net, tf.sqrt(tf.reduce_mean( net**2 , axis = 3, keepdims= True) + 1e-8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NSOpuJIomrp0",
        "colab": {}
      },
      "source": [
        "def minibatch_std_dev( net ):\n",
        "\n",
        "    y = net - tf.reduce_mean( net, axis = 0, keepdims= True  )\n",
        "    y = tf.reduce_mean(  y**2, axis= 0, keepdims= False )\n",
        "    y = tf.sqrt( y + 1e-8 )\n",
        "    std_dev = tf.reduce_mean( y) * tf.ones( [tf.shape(net)[0], tf.shape(net)[1], tf.shape(net)[2], 1 ] )\n",
        "    net = tf.concat( [ net, std_dev ], axis = 3 )\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N84AI7qQS4yU",
        "colab": {}
      },
      "source": [
        "def activate(net, activation, alpha=None ):\n",
        "    if activation == \"leaky_relu\":\n",
        "        if alpha is None:\n",
        "            alpha = 0.2\n",
        "        net = tf.nn.leaky_relu(net, alpha= alpha )\n",
        "    elif activation == \"relu\":\n",
        "        net = tf.nn.relu(net )\n",
        "    elif activation == \"sigmoid\":\n",
        "        net = tf.nn.sigmoid(net)\n",
        "    else:\n",
        "        print(\"unrecognized activation!\")\n",
        "        exit(1)\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NKytbQGUdpvx",
        "colab": {}
      },
      "source": [
        "def conv_layer(net,  trainable ,  units  , kernel, strides,  dropout_rate = 0,  activation = None , alpha= None, var_name = None  ):\n",
        "    if not FLAGS.is_equalized_learning_rate:\n",
        "        net = tf.layers.Conv2D(units, kernel, strides,\"same\", trainable= trainable)(net)\n",
        "    else: # this is for equalized learning rate implementation\n",
        "        n_feats_in = net.get_shape().as_list()[-1]\n",
        "        fan_in = kernel * kernel * n_feats_in\n",
        "        c = tf.constant(np.sqrt(2. / fan_in), dtype=tf.float32)\n",
        "        kernel_init = tf.random_normal_initializer(stddev=1.)\n",
        "        w_shape = [kernel, kernel, n_feats_in, units]\n",
        "        w = tf.get_variable('kernel_'+var_name , shape=w_shape, initializer=kernel_init, trainable= trainable)\n",
        "        w = c * w\n",
        "        net = tf.nn.conv2d(net, w, strides, padding=\"SAME\" )\n",
        "        b = tf.get_variable('bias_'+var_name, [units], initializer=tf.constant_initializer(0.), trainable= trainable  )\n",
        "        net = tf.nn.bias_add(net, b)\n",
        "    if activation is not None:\n",
        "        net = activate( net, activation , alpha )\n",
        "    if dropout_rate >0:\n",
        "        net = tf.layers.Dropout(rate=dropout_rate)( net , training = trainable )\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w1bAGC_Ot37n",
        "colab": {}
      },
      "source": [
        "def toRGB( net, trainable, units, scope = \"toRGB\"):\n",
        "    with tf.variable_scope(scope, reuse= tf.AUTO_REUSE ):\n",
        "        net = conv_layer( net, trainable, units, 1, (1,1), 0 , None , None, \"toRGB\" )\n",
        "        return net\n",
        "\n",
        "def fromRGB( net, trainable, units, scope = \"fromRGB\" ):\n",
        "    with tf.variable_scope(scope, reuse= tf.AUTO_REUSE ):\n",
        "        net = conv_layer( net, trainable, units, 1, (1,1), 0 , \"leaky_relu\" , 0.2, \"fromRGB\" )\n",
        "        return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KkD7XeJahOpg",
        "colab": {}
      },
      "source": [
        "def generator( z, trainable = True, scope = \"generator\", dataset = \"celeba\", stage = 0, alpha = 0 ):\n",
        "    with tf.variable_scope( scope, reuse= tf.AUTO_REUSE ):\n",
        "        if dataset == \"celeba\":\n",
        "            size_list =  FLAGS.size_list\n",
        "            channel_list =   FLAGS.generator_channel_list \n",
        "            with tf.variable_scope( \"stage0\", reuse= tf.AUTO_REUSE):\n",
        "                net = tf.layers.Dense( 4*4*512, activation=tf.nn.leaky_relu, trainable= trainable )(z)\n",
        "                net = tf.reshape( net, [ tf.shape(net)[0], size_list[0] ,size_list[0], channel_list[0] ] )\n",
        "                net = pixelnorm(conv_layer( net, trainable, channel_list[0], 3, (1,1), 0 , \"leaky_relu\", 0.2, \"conv1\" ))\n",
        "                x = toRGB( net, trainable, 3 )\n",
        "\n",
        "            for stg in range(1, stage+1):\n",
        "                if stg == stage:\n",
        "                    previous_x = x\n",
        "                with tf.variable_scope( \"stage%d\"%(stg), reuse= tf.AUTO_REUSE ):\n",
        "                    net = tf.image.resize_bilinear( net, [size_list[stg], size_list[stg]], align_corners= True )\n",
        "                    net = pixelnorm(conv_layer( net, trainable, channel_list[stg], 3, (1,1), 0, \"leaky_relu\", 0.2 , \"conv1\"  ))\n",
        "                    net = pixelnorm(conv_layer( net, trainable, channel_list[stg], 3, (1,1), 0, \"leaky_relu\", 0.2 , \"conv2\" ))\n",
        "                    x = toRGB( net, trainable, 3 )\n",
        "            if stage > 0:\n",
        "                x = (1-alpha) * tf.image.resize_bilinear(previous_x, [ size_list[stage], size_list[stage]], align_corners= True ) + alpha * x\n",
        "            \n",
        "            return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZdDTzX2rd8uV",
        "colab": {}
      },
      "source": [
        "def discriminator( x,  trainable= True,scope = \"discriminator\", dataset = \"celeba\", stage = 0, alpha = 0 ):\n",
        "    with tf.variable_scope( scope, reuse = tf.AUTO_REUSE  ):\n",
        "        if dataset == \"celeba\":\n",
        "            size_list = FLAGS.size_list\n",
        "            in_channel_list =  FLAGS.discriminator_in_channel_list  \n",
        "            out_channel_list = FLAGS.discriminator_out_channel_list \n",
        "            if stage > 0:\n",
        "                with tf.variable_scope( \"stage%d\"%(stage), reuse=tf.AUTO_REUSE ):\n",
        "                    net = fromRGB( x, trainable, in_channel_list[stage])\n",
        "                    net = conv_layer( net, trainable, in_channel_list[stage], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv1\" )\n",
        "                    net = conv_layer( net, trainable, out_channel_list[stage], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv2\")\n",
        "                    net = tf.layers.AveragePooling2D( 2,2,\"same\" )(net)\n",
        "                with tf.variable_scope( \"stage%d\"%(stage-1), reuse=tf.AUTO_REUSE ):\n",
        "                    net_1 = fromRGB( tf.layers.AveragePooling2D(2,2,\"same\")(x), trainable, in_channel_list[stage-1] )\n",
        "                net = (1-alpha) * net_1 + alpha * net\n",
        "            else:\n",
        "                with tf.variable_scope( \"stage0\", reuse= tf.AUTO_REUSE):\n",
        "                    net = fromRGB( x, trainable, in_channel_list[0])\n",
        "            \n",
        "            for stg in range( stage-1, 0, -1 ):\n",
        "                with tf.variable_scope( \"stage%d\"%(stg), reuse= tf.AUTO_REUSE ):\n",
        "                    net = conv_layer( net, trainable, in_channel_list[stg], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv1\")\n",
        "                    net = conv_layer( net, trainable, out_channel_list[stg], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv2\")\n",
        "                    net = tf.layers.AveragePooling2D( 2,2,\"same\" )(net)\n",
        "\n",
        "            with tf.variable_scope( \"stage0\", reuse= tf.AUTO_REUSE):\n",
        "                net = minibatch_std_dev(net)\n",
        "                net = conv_layer( net, trainable, in_channel_list[0], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv1\")\n",
        "                net = tf.layers.Flatten()(net)\n",
        "                net = tf.layers.Dense( out_channel_list[0], activation= tf.nn.leaky_relu, trainable= trainable)(net)\n",
        "                net = tf.layers.Dense(1, trainable= trainable )(net)\n",
        "\n",
        "        return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F3S8IYohZrwX",
        "colab": {}
      },
      "source": [
        "def encoder( x,  trainable= True,scope = \"encoder\", dataset = \"celeba\", stage = 0, alpha = 0 ):\n",
        "    with tf.variable_scope( scope, reuse = tf.AUTO_REUSE  ):\n",
        "        if dataset == \"celeba\":\n",
        "            size_list = FLAGS.size_list\n",
        "            in_channel_list =  FLAGS.encoder_in_channel_list  \n",
        "            out_channel_list = FLAGS.encoder_out_channel_list\n",
        "            if stage > 0:\n",
        "                with tf.variable_scope( \"stage%d\"%(stage), reuse=tf.AUTO_REUSE ):\n",
        "                    net = fromRGB( x, trainable, in_channel_list[stage])\n",
        "                    net = conv_layer( net, trainable, in_channel_list[stage], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv1\")\n",
        "                    net = conv_layer( net, trainable, out_channel_list[stage], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv2\")\n",
        "                    net = tf.layers.AveragePooling2D( 2,2,\"same\" )(net)\n",
        "                with tf.variable_scope( \"stage%d\"%(stage-1), reuse=tf.AUTO_REUSE ):\n",
        "                    net_1 = fromRGB( tf.layers.AveragePooling2D(2,2,\"same\")(x), trainable, in_channel_list[stage-1])\n",
        "                    net = (1-alpha) * net_1 + alpha * net\n",
        "            else:\n",
        "                with tf.variable_scope( \"stage0\", reuse= tf.AUTO_REUSE):\n",
        "                    net = fromRGB( x, trainable, in_channel_list[0])\n",
        "            \n",
        "            for stg in range( stage-1, 0, -1 ):\n",
        "                with tf.variable_scope( \"stage%d\"%(stg), reuse= tf.AUTO_REUSE ):\n",
        "                    net = conv_layer( net, trainable, in_channel_list[stg], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv1\")\n",
        "                    net = conv_layer( net, trainable, out_channel_list[stg], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv2\")\n",
        "                    net = tf.layers.AveragePooling2D( 2,2,\"same\" )(net)\n",
        "\n",
        "            with tf.variable_scope( \"stage0\", reuse= tf.AUTO_REUSE):\n",
        "                net = conv_layer( net, trainable, in_channel_list[0], 3, (1,1), 0, \"leaky_relu\", 0.2, \"conv1\")\n",
        "                net = tf.layers.Flatten()(net)\n",
        "                net = tf.layers.Dense( out_channel_list[0], activation= tf.nn.leaky_relu, trainable= trainable)(net)\n",
        "                net = tf.layers.Dense( FLAGS.z_dim , trainable= trainable )(net)\n",
        "            \n",
        "        return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "em2xHWILFfMt"
      },
      "source": [
        "### load model weights of the previous resolutions for initialization of the model of the current resolustion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Djx72ezcPb65",
        "colab": {}
      },
      "source": [
        "def init_weights(scope_name, path):\n",
        "    if path == None:\n",
        "        return\n",
        "    # look for checkpoint\n",
        "    model_path = tf.train.latest_checkpoint(path)\n",
        "    initializer_fn = None\n",
        "    if model_path:\n",
        "        # only restore variables in the scope_name scope\n",
        "        variables_to_restore = slim.get_variables_to_restore(include=scope_name)\n",
        "        # Create the saver which will be used to restore the variables.\n",
        "        initializer_fn = slim.assign_from_checkpoint_fn(model_path, variables_to_restore)\n",
        "    else:\n",
        "        print(\"could not find the fine tune ckpt at {}\".format(path))\n",
        "        exit()\n",
        "    def InitFn(scaffold,sess):\n",
        "        initializer_fn(sess)\n",
        "    return InitFn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HiewctTEVQsf"
      },
      "source": [
        "## Building the model and define the behavior of training, evaluation and prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TDnPK26tVW-N",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "    # Args:\n",
        "    # features: This is the x-arg from the input_fn.\n",
        "    # labels:   This is the y-arg from the input_fn,\n",
        "    #           see e.g. train_input_fn for these two.\n",
        "    # mode:     Either TRAIN, EVAL, or PREDICT\n",
        "    # params:   User-defined hyper-parameters, e.g. learning-rate.\n",
        "    \n",
        "    dataset = params[\"dataset\"]\n",
        "    lr = params[\"learning_rate\"]\n",
        "    stage = params[\"stage\"]\n",
        "\n",
        "    \"\"\" Part I. create the model networks\"\"\"\n",
        "    is_train = mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "    if is_train:\n",
        "        x = features[\"x\"]\n",
        "    else:\n",
        "        x = features\n",
        "\n",
        "    # here x has a range [-1, 1]\n",
        "    global_step = tf.train.get_global_step()\n",
        "    # alpha_for_transition for stage0 is actually never used since stage0 doesnot require resolution transition\n",
        "    alpha_for_transition = tf.clip_by_value(  tf.cast(global_step , tf.float32 ) / (FLAGS.train_steps_list[stage]/2), 0, 1  )\n",
        "\n",
        "    size_list = FLAGS.size_list\n",
        "    if stage > 0:\n",
        "        x_low =  tf.image.resize_bilinear( x, [size_list[stage-1],size_list[stage-1]], align_corners= True)\n",
        "        x_low_up =  tf.image.resize_bilinear( x_low, [size_list[stage],size_list[stage]], align_corners= True  )\n",
        "        x = alpha_for_transition * x + ( 1-alpha_for_transition )* x_low_up\n",
        "    \n",
        "    random_z = tf.random.normal( [tf.shape(x)[0], FLAGS.z_dim ]  )\n",
        "    gen_x = generator( random_z, trainable= is_train , dataset= dataset, stage=stage, alpha= alpha_for_transition )\n",
        "    dis_x = discriminator( x, trainable= is_train, dataset=dataset, stage=stage, alpha= alpha_for_transition  )\n",
        "    dis_gen_x = discriminator( gen_x, trainable= is_train, dataset=dataset, stage=stage , alpha= alpha_for_transition  )\n",
        "    # for the encoder part\n",
        "    recon_z = encoder( gen_x, trainable = is_train, dataset = dataset, stage=stage ,  alpha= alpha_for_transition )\n",
        "    # for image reconstruction\n",
        "    enc_z = encoder( x, trainable= False, dataset= dataset, stage=stage, alpha= alpha_for_transition  )\n",
        "    recon_x = generator( enc_z, trainable= False, dataset= dataset, stage=stage , alpha= alpha_for_transition )\n",
        "\n",
        "    # This is used to compute the gradient penalty\n",
        "    epsilon = tf.random.uniform( [ tf.shape(x)[0],1,1,1 ], minval=0, maxval= 1 )\n",
        "    interp_x = epsilon * x + (1-epsilon) * gen_x\n",
        "    dis_interp_x = discriminator( interp_x, trainable= is_train, dataset=dataset, stage= stage , alpha= alpha_for_transition )\n",
        "    gradient_x = tf.gradients( dis_interp_x, [ interp_x ]  )[0]\n",
        "    gradient_penalty = tf.square( tf.sqrt( tf.reduce_sum( tf.square(gradient_x ),[1,2,3] ) + 1e-6) - 1  )\n",
        "    LAMBDA = 10\n",
        "\n",
        "    e_drift = 1e-3\n",
        "    dis_drift_loss = e_drift * dis_x**2\n",
        "\n",
        "    \"\"\"load parameters from previous model\"\"\"\n",
        "    # tf.train.init_from_checkpoint also require load the Adam optimizer's parameter, which may cause some error information sometimes, here we use scaffold_fn\n",
        "    # same as init_from_checkpoint, scaffold_fn will be only executed once when there is no checkpoints for current model. If there is some checkpoints for current model,\n",
        "    # this function is not executed, which means that it will not override some existing checkpoints' parameters (under testing ...)\n",
        "    scaffold_fn = None\n",
        "    if stage >0:\n",
        "        previous_model_dir = FLAGS.model_dir+\"stage%d\"%(stage-1)\n",
        "        scope_name_list =[]\n",
        "        for stg in range( stage ):\n",
        "            scope_name_list += [ \"discriminator/stage%d/\"%(stg), \"generator/stage%d/\"%(stg), \"encoder/stage%d/\"%(stg) ]\n",
        "        def scaffold_fn():\n",
        "            return tf.train.Scaffold( init_fn=  init_weights( scope_name_list, previous_model_dir )  )\n",
        "    \n",
        "\n",
        "    \"\"\"Part II. define the loss and relative parameters for mode == TRAIN/EVAL/PREDICT\"\"\"\n",
        "    ## compute loss\n",
        "    loss_dis = dis_gen_x  - dis_x + LAMBDA * gradient_penalty + dis_drift_loss\n",
        "    loss_gen = - dis_gen_x\n",
        "    W_dis = dis_x - dis_gen_x\n",
        "    loss_recon_z = tf.reduce_sum( tf.square(random_z - recon_z), [1])\n",
        "    loss_reg_std_z = tf.abs( tf.math.reduce_std( recon_z ) -1 )\n",
        "\n",
        "    ## operations for the training mode, define the optimizer, and reconfig it using tpu.CrossShardOptimizer\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        # g_w = features[\"g_w\"]\n",
        "        loss_dis = tf.reduce_mean( loss_dis   )  \n",
        "        loss_gen = tf.reduce_mean( loss_gen   )  #  *g_w)\n",
        "        W_dis = tf.reduce_mean(W_dis)\n",
        "        loss_recon_z = tf.reduce_mean(loss_recon_z  ) #  *g_w)\n",
        "        loss_reg_std_z = loss_reg_std_z  # *g_w\n",
        "\n",
        "        # Define the optimizer\n",
        "        d_optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=0, beta2= 0.99 )\n",
        "        g_optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=0, beta2= 0.99 )\n",
        "        e_optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=0, beta2= 0.99 )\n",
        "        e2g_optimizer = tf.train.AdamOptimizer(learning_rate=lr/5, beta1=0, beta2= 0.99 )\n",
        "\n",
        "\n",
        "        if FLAGS.use_tpu:\n",
        "            d_optimizer = tf.tpu.CrossShardOptimizer(d_optimizer)\n",
        "            g_optimizer = tf.tpu.CrossShardOptimizer(g_optimizer)\n",
        "            e_optimizer = tf.tpu.CrossShardOptimizer(e_optimizer)\n",
        "            e2g_optimizer = tf.tpu.CrossShardOptimizer(e2g_optimizer)\n",
        "\n",
        "        with tf.control_dependencies( tf.get_collection( tf.GraphKeys.UPDATE_OPS )):\n",
        "            d_op = d_optimizer.minimize( loss = loss_dis, var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\\\n",
        "                                           scope=\"discriminator\")  )\n",
        "            g_op = g_optimizer.minimize(loss = loss_gen,  var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\\\n",
        "                                           scope=\"generator\"),global_step= global_step )\n",
        "            e_op = e_optimizer.minimize(loss = loss_recon_z + loss_reg_std_z ,  var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\\\n",
        "                                           scope=\"encoder\"))\n",
        "            e2g_op = e2g_optimizer.minimize(loss = loss_recon_z * 1e-2 ,  var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\\\n",
        "                                           scope=\"generator\"))\n",
        "            # increment_step = tf.assign_add( tf.train.get_or_create_global_step(), 1)\n",
        "            train_op = tf.group( [ d_op , g_op, e_op, e2g_op ] )\n",
        "            # STRANGE: the definition of spec cannot be putinto with ...\n",
        "            spec= tf.estimator.tpu.TPUEstimatorSpec(mode=mode, loss= W_dis ,train_op= train_op,  scaffold_fn = scaffold_fn  )\n",
        "    ## for EVAL mode, the parameters eval_metrics takes a tuple or list of two elements. The first element is a callable function,\n",
        "    ## The second element is a list of parameters. The return value of the callable function will be shown in the evaluatio results\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "        spec = tf.estimator.tpu.TPUEstimatorSpec(mode=mode, loss= tf.reduce_mean( W_dis), eval_metrics=(metric_fn, [loss_gen, loss_dis, W_dis, loss_recon_z ] ) )\n",
        "    \n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        input_z = tf.random.normal( shape=( tf.shape(random_z) ) )\n",
        "        predictions = { \"generated_images\":  gen_x ,\n",
        "                        \"input_images\": x,\n",
        "                        \"reconstructed_images\": recon_x\n",
        "                          }\n",
        "        spec= tf.estimator.tpu.TPUEstimatorSpec( mode = mode, predictions = predictions )\n",
        "        return spec\n",
        "\n",
        "    \n",
        "    return spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9WoEKTM3WhvY"
      },
      "source": [
        "## Create the TPUEstimator entity, and run the train / evaluate/ predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovokb2YRWPG8",
        "outputId": "c9ea3d25-2146-47ce-c8ce-a45bcdd821cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_for_stage = []\n",
        "\n",
        "for stg in range( len(FLAGS.size_list) ):\n",
        "    run_config = tf.estimator.tpu.RunConfig(\n",
        "        model_dir=FLAGS.model_dir+\"stage%d\"%(stg),\n",
        "        cluster=tf.distribute.cluster_resolver.TPUClusterResolver(FLAGS.tpu),\n",
        "        session_config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True),\n",
        "        tpu_config=tf.estimator.tpu.TPUConfig(FLAGS.iterations_per_loop),\n",
        "        )\n",
        "\n",
        "    model = tf.estimator.tpu.TPUEstimator(\n",
        "                               model_fn=model_fn,\n",
        "                               params = {\"learning_rate\": FLAGS.lr_list[stg]  , \"dataset\": FLAGS.dataset, \"stage\":stg },\n",
        "                               config = run_config,\n",
        "                               use_tpu= FLAGS.use_tpu,\n",
        "                               train_batch_size=FLAGS.batch_size_list[stg]  ,\n",
        "                               eval_batch_size=FLAGS.batch_size_list[stg] ,\n",
        "                               predict_batch_size= 64,\n",
        "                              ) \n",
        "    model_for_stage.append(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://reverse-vae-celeba-hq/model/stage0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.76.77.186:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7450a38278>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.76.77.186:8470', '_evaluation_master': 'grpc://10.76.77.186:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f7450a2ce48>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://reverse-vae-celeba-hq/model/stage1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.76.77.186:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7450a885c0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.76.77.186:8470', '_evaluation_master': 'grpc://10.76.77.186:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f7450a38550>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://reverse-vae-celeba-hq/model/stage2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.76.77.186:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7450a88ac8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.76.77.186:8470', '_evaluation_master': 'grpc://10.76.77.186:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f7450a88978>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://reverse-vae-celeba-hq/model/stage3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.76.77.186:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f74509f0080>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.76.77.186:8470', '_evaluation_master': 'grpc://10.76.77.186:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f7450a88cc0>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://reverse-vae-celeba-hq/model/stage4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.76.77.186:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f74509f04e0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.76.77.186:8470', '_evaluation_master': 'grpc://10.76.77.186:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f74509f0198>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://reverse-vae-celeba-hq/model/stage5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.76.77.186:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f74509f06d8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.76.77.186:8470', '_evaluation_master': 'grpc://10.76.77.186:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f74509f03c8>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://reverse-vae-celeba-hq/model/stage6', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.76.77.186:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f74509f0898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.76.77.186:8470', '_evaluation_master': 'grpc://10.76.77.186:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f74e4c959e8>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://reverse-vae-celeba-hq/model/stage7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.76.77.186:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7450a38898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.76.77.186:8470', '_evaluation_master': 'grpc://10.76.77.186:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f74e4cb4390>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "odBUseLSWYtl"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t8dY5URVWa5x",
        "outputId": "73e1601d-28c0-46c5-f97d-a4925f78ed8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for stg in range(len(FLAGS.size_list)):\n",
        "    model = model_for_stage[stg]\n",
        "    model.train( input_fn = lambda params: train_input_fn( FLAGS.dataset, stg, params[\"batch_size\"] ), max_steps=FLAGS.train_steps_list[stg]  )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.76.77.186:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 7055712508931361655)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8594660463716365610)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 17415441877167209408)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 825608924856497278)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8506710606270258419)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8658283002671905074)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9490073253169238025)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7257476816552045060)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5247709431295990434)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 2611069170936783025)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6562678006498084896)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into gs://reverse-vae-celeba-hq/model/stage0/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 2)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 4)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 6)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 8)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 10)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 12)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 14)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 16)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 18)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 20)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 22)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 24)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 26)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 28)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 30)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 32)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 34)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 36)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 38)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 40)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 42)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 44)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 46)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 48)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 50)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 52)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 54)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 56)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 58)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 60)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 62)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 64)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 66)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 68)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 70)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 72)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 74)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 76)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 78)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 80)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 82)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 84)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 86)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 88)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 90)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 92)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 94)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 96)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 98)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 100)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 102)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 104)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 106)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 108)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 110)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 112)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 114)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 116)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 118)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 120)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 122)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 124)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 126)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 128)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 130)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 132)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 134)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 136)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 138)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 140)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 142)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 144)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 146)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 148)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 150)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 152)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 154)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 156)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 158)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 160)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 162)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 164)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 166)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 168)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 170)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 172)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 174)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 176)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gsvWROSqg_Hc"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2-TdmDsUtAN8",
        "colab": {}
      },
      "source": [
        "stage = 0\n",
        "eval_result = model.evaluate(input_fn=lambda params: eval_input_fn( FLAGS.dataset,stage,params[\"batch_size\"]), steps = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WnbsEWaHgn99",
        "colab": {}
      },
      "source": [
        "eval_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PFd5J_9HhJN8"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "adgrl-YurFb5",
        "colab": {}
      },
      "source": [
        "stage =0\n",
        "model = model_for_stage[stage]\n",
        "\n",
        "pred_results = model.predict( input_fn=lambda params: predict_input_fn(FLAGS.dataset, stage, params[\"batch_size\"] ) )\n",
        "images = [ [result[\"generated_images\"], result[\"input_images\"] ,result[\"reconstructed_images\"]  ] for result in pred_results  ]\n",
        "\n",
        "generated_images = [ im[0] for im in images ]\n",
        "generated_images = np.array( generated_images)\n",
        "\n",
        "input_images = [ im[1] for im in images ]\n",
        "input_images = np.array( input_images)\n",
        "\n",
        "reconstructed_images = [ im[2] for im in images ]\n",
        "reconstructed_images = np.array( reconstructed_images)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-kMBS5JwrdaS",
        "colab": {}
      },
      "source": [
        "grid_imges = np.squeeze( make_grid(add_padding(generated_images[np.random.choice(generated_images.shape[0], 64, replace=False)]) ))\n",
        "imageio.imwrite(\"random_generate.png\",grid_imges )\n",
        "if len(grid_imges.shape) <3:\n",
        "    plt.gray()\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(grid_imges)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7dzrGGeliw0n",
        "colab": {}
      },
      "source": [
        "randidx = np.random.choice( input_images.shape[0], 64, replace= False  )\n",
        "grid_imges = np.squeeze( make_grid(np.concatenate([ add_padding(input_images[ randidx]), add_padding(reconstructed_images[randidx]) ], axis =2 ) ) )\n",
        "imageio.imwrite(\"recon.png\", grid_imges)\n",
        "plt.gray()\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(grid_imges)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JXSKBnE56r2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}